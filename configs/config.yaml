model: informer  # model of experiment, options: [informer, informerstack, informerlight(TBD)]

data: MEWS  # data
# root_path_data: "\\chansey.umcn.nl\\pelvis\\projects\\romainandre\\Datasets\\AI-EWS"  # root path for data loading
# root_path_data: /container/data # root path for data loading
root_path_data: Z:/Datasets/AI-EWS  # root path for data loading
root_path_save: Z:/Datasets/AI-EWS/exp  # root path for saving checkpoints and logs
# data_path: vitals_data.csv  
data_path:
  vitals: vitals_data.csv # data file #timeseries_for_informer.csv
  mappings: mapping_everything.csv  # mapping data file
  admissions: admissions_data_prepped.csv  # static data file
  antibiotics: filtered_antibiotics.csv  # infections data file
features: M  # forecasting task, options:[M, S, MS]; M: multivariate → predict multivariate, S: univariate → predict univariate, MS: multivariate → predict univariate
target: HR  # target feature in S or MS task
freq: t  # freq for time features encoding, e.g. [s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly]; can also use 15min, 3h, etc.
checkpoints_path: informer_checkpoints  # location of model checkpoints (relative to root_path_save)
# TODO add and make sure it's processed properly:
# resume: True # load the last checkpoint with the same configuration
logging_path: logs # location of logs (relative to root_path_save)
save_every: 10 # save the model every x epochs

# TODO: currently, the value to predict is fetched from the next row
seq_len: 48    # input sequence length of the Informer encoder (dim 1)
label_len: 24  # start-token length of the Informer decoder (dim 2)
pred_len: 12   # prediction sequence length (dim 3). Informer decoder input = concat[start token series(label_len), zero padding series(pred_len)]
# Informer decoder input: concat[start token series(label_len), zero padding series(pred_len)]

enc_in: 6  # encoder input size
dec_in: 7 # decoder input size
c_out: 7   # output size
factor: 5  # ProbSparse attention factor
d_model: 512  # dimension of the model
n_heads: 8  # number of attention heads
e_layers: 4 #2  # number of encoder layers #TODO: More if cluster
d_layers: 4 #2  # number of decoder layers #TODO: More if cluster
d_ff: 2048  # dimension of FCN in the model
dropout: 0.05  # dropout rate
attn: prob  # attention type in encoder, options: [prob, full]
embed: timeF  # time features encoding, options: [timeF, fixed, learned]
activation: gelu  # activation function
distil: true  # whether to use distilling in the encoder
output_attention: false  # whether to output attention in the encoder
mix: true
padding: 0

#Conditionning
condition: true #Whether to condition the model on the static variables
n_cond_num_in: 1 #How many numerical variables to condition on
n_cond_num_out: 2 #Embedding size for numerical variables
n_cond_cat_in: "2,32,605" #How many categories for each categorical variable
n_cond_cat_out: "4,16,16" #Embedding size for each categorical variable

# s_layers is often used for InformerStack or advanced configs (optional)
s_layers: "3,2,1"

# Training configs
batch_size: 256 #363
learning_rate: 0.0002
loss: mse #mse
loss_category: crossentropy
loss_alpha: 5 #how much the mse weights
lradj: type1
use_amp: false  # whether to use automatic mixed precision training

num_workers: 0
itr: 1
train_epochs: 24
patience: 3
des: exp
do_predict: false #TODO: what
inverse: false #TODO: what

# GPU configs
use_gpu: true
gpu: 0

use_multi_gpu: false
devices: "0,1,2,3"

cols: null  # specify certain cols if needed; default null means "use all"
